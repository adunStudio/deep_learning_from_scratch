{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(10, 1, 28, 28) # 무작위로 데이터 생성\n",
    "print(x. shape) # (10, 1, 28, 28)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x[0].shape) # (1, 28, 28)\n",
    "print(x[1].shape) # (1, 28, 28)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x[0][0].shape) # (28, 28)\n",
    "print(x[0, 0].shape) # (28, 28)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from common.util import im2col"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%ㄹ\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) # (9, 75)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) # (90, 750)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995351489324576\n",
      "=== epoch:1, train acc:0.221, test acc:0.227 ===\n",
      "train loss:2.296583649518938\n",
      "train loss:2.2923550561862798\n",
      "train loss:2.289402693637427\n",
      "train loss:2.2786988507590586\n",
      "train loss:2.2706642031427826\n",
      "train loss:2.2426646069381753\n",
      "train loss:2.2478901583905206\n",
      "train loss:2.2158588614396373\n",
      "train loss:2.204881197884935\n",
      "train loss:2.1488510469808815\n",
      "train loss:2.1556429721468917\n",
      "train loss:2.086836948825445\n",
      "train loss:2.0407862566670665\n",
      "train loss:1.9722249083492804\n",
      "train loss:1.9417092718648956\n",
      "train loss:1.8903277297114545\n",
      "train loss:1.8343694862404154\n",
      "train loss:1.6693195471198465\n",
      "train loss:1.615513073070281\n",
      "train loss:1.489291572865623\n",
      "train loss:1.5258466080006394\n",
      "train loss:1.3354770364801944\n",
      "train loss:1.4541347024330742\n",
      "train loss:1.1907807114351743\n",
      "train loss:1.2256058360180142\n",
      "train loss:1.0227706639926286\n",
      "train loss:1.0258073351156036\n",
      "train loss:0.9663588364419563\n",
      "train loss:0.9762813473816198\n",
      "train loss:0.9839011720755542\n",
      "train loss:0.8180743271783338\n",
      "train loss:0.7484226335339674\n",
      "train loss:0.8391945300871663\n",
      "train loss:0.6882067924893611\n",
      "train loss:0.7140984557567728\n",
      "train loss:0.734609636845581\n",
      "train loss:0.8630507534333103\n",
      "train loss:0.7347933753802893\n",
      "train loss:0.6134408241918674\n",
      "train loss:0.8339128244156364\n",
      "train loss:0.6645044738825522\n",
      "train loss:0.7935811740944194\n",
      "train loss:0.6679573834836675\n",
      "train loss:0.5258390389327441\n",
      "train loss:0.7169324840816705\n",
      "train loss:0.6345917328051914\n",
      "train loss:0.5559575108963947\n",
      "train loss:0.5779790176579819\n",
      "train loss:0.6129682469286295\n",
      "train loss:0.7047096822135268\n",
      "train loss:0.934776760249006\n",
      "train loss:0.46263440062003214\n",
      "train loss:0.5610965982221559\n",
      "train loss:0.6377233067916537\n",
      "train loss:0.4321221469075012\n",
      "train loss:0.5806929409588247\n",
      "train loss:0.4576014354318072\n",
      "train loss:0.5532974124200662\n",
      "train loss:0.5649296682065162\n",
      "train loss:0.5015975552815202\n",
      "train loss:0.5047522017567582\n",
      "train loss:0.5177027825875998\n",
      "train loss:0.5613543328707571\n",
      "train loss:0.43872332694062444\n",
      "train loss:0.4888159305942788\n",
      "train loss:0.6129021895679541\n",
      "train loss:0.42859869826072233\n",
      "train loss:0.41337224291533525\n",
      "train loss:0.4610379074596102\n",
      "train loss:0.4631476865171687\n",
      "train loss:0.46295885549603605\n",
      "train loss:0.27689130785460925\n",
      "train loss:0.4799562594315559\n",
      "train loss:0.3270058334609993\n",
      "train loss:0.5687640099473701\n",
      "train loss:0.4033044057043456\n",
      "train loss:0.3977902423402861\n",
      "train loss:0.3937728004440731\n",
      "train loss:0.37824680457752996\n",
      "train loss:0.36639945124653955\n",
      "train loss:0.40390751080788884\n",
      "train loss:0.6183791799681518\n",
      "train loss:0.4562360452038963\n",
      "train loss:0.4445523925164756\n",
      "train loss:0.39374101117042115\n",
      "train loss:0.5026814917263731\n",
      "train loss:0.4097230161581264\n",
      "train loss:0.4422512763428361\n",
      "train loss:0.5079509858076795\n",
      "train loss:0.45458251577360875\n",
      "train loss:0.36843117656435953\n",
      "train loss:0.4242549566369934\n",
      "train loss:0.6263877139719591\n",
      "train loss:0.38614934262203077\n",
      "train loss:0.4573394565640083\n",
      "train loss:0.4146310119114385\n",
      "train loss:0.3744240762318342\n",
      "train loss:0.36362838656518826\n",
      "train loss:0.4833019926865488\n",
      "train loss:0.31997077488158754\n",
      "train loss:0.2959348083885251\n",
      "train loss:0.3445680089429361\n",
      "train loss:0.33494346792278284\n",
      "train loss:0.39049336286463054\n",
      "train loss:0.41363228053857404\n",
      "train loss:0.3378517677817748\n",
      "train loss:0.39557573301390875\n",
      "train loss:0.4839164660899473\n",
      "train loss:0.3290315976396306\n",
      "train loss:0.3178859205812958\n",
      "train loss:0.32211103416190495\n",
      "train loss:0.3456952362816132\n",
      "train loss:0.48365926576368024\n",
      "train loss:0.34027311432618296\n",
      "train loss:0.43348787763949204\n",
      "train loss:0.4490874192185694\n",
      "train loss:0.4401698558801987\n",
      "train loss:0.33333093729420915\n",
      "train loss:0.3106924309187409\n",
      "train loss:0.38320892482737845\n",
      "train loss:0.2456919950324144\n",
      "train loss:0.3518232622383532\n",
      "train loss:0.3952781378893206\n",
      "train loss:0.34364138893478013\n",
      "train loss:0.2352166537975614\n",
      "train loss:0.4016976745379385\n",
      "train loss:0.4489671007827541\n",
      "train loss:0.2879688340903994\n",
      "train loss:0.17466896130739676\n",
      "train loss:0.3644470858694595\n",
      "train loss:0.2584359528918662\n",
      "train loss:0.6086444790208676\n",
      "train loss:0.3616233378233982\n",
      "train loss:0.23589443557383952\n",
      "train loss:0.22300213142135053\n",
      "train loss:0.3593670037221953\n",
      "train loss:0.28759140037817543\n",
      "train loss:0.4931124605863022\n",
      "train loss:0.40053560324862936\n",
      "train loss:0.3960062398731127\n",
      "train loss:0.2738891474180552\n",
      "train loss:0.2940334714559897\n",
      "train loss:0.24675387506893653\n",
      "train loss:0.3551764274210037\n",
      "train loss:0.3502783202890427\n",
      "train loss:0.29586193670656546\n",
      "train loss:0.33630840298472614\n",
      "train loss:0.25301458773858576\n",
      "train loss:0.30093953322787326\n",
      "train loss:0.25826645109517216\n",
      "train loss:0.30498829260712595\n",
      "train loss:0.34035523277612006\n",
      "train loss:0.5777843460855308\n",
      "train loss:0.328977212989558\n",
      "train loss:0.27920188474199004\n",
      "train loss:0.4617063331669793\n",
      "train loss:0.34183277584025545\n",
      "train loss:0.1765533651391628\n",
      "train loss:0.2679426318036977\n",
      "train loss:0.32356170423785796\n",
      "train loss:0.4882168477228936\n",
      "train loss:0.2835554235492619\n",
      "train loss:0.24129649147915788\n",
      "train loss:0.4842486719955407\n",
      "train loss:0.32746859764193814\n",
      "train loss:0.25444684695481945\n",
      "train loss:0.2718822539933015\n",
      "train loss:0.3342150377177778\n",
      "train loss:0.288712333154298\n",
      "train loss:0.18051842938643564\n",
      "train loss:0.16878336850038522\n",
      "train loss:0.31658061743701144\n",
      "train loss:0.23182075783143194\n",
      "train loss:0.45764922916047385\n",
      "train loss:0.38105949266058486\n",
      "train loss:0.453600721303484\n",
      "train loss:0.2907496750015728\n",
      "train loss:0.27208556969400916\n",
      "train loss:0.24642027992550958\n",
      "train loss:0.2677442514103307\n",
      "train loss:0.3442961500224596\n",
      "train loss:0.25114323068089744\n",
      "train loss:0.3231054936843993\n",
      "train loss:0.2883733217641921\n",
      "train loss:0.36615969635010315\n",
      "train loss:0.3538346609997289\n",
      "train loss:0.193106020968991\n",
      "train loss:0.30760882368313913\n",
      "train loss:0.17649548806376006\n",
      "train loss:0.4302051343054181\n",
      "train loss:0.2795773826458991\n",
      "train loss:0.3898424274240325\n",
      "train loss:0.4583030206296235\n",
      "train loss:0.2906108088468853\n",
      "train loss:0.17779276122211848\n",
      "train loss:0.36486370449544653\n",
      "train loss:0.4684978121269955\n",
      "train loss:0.32935725645881975\n",
      "train loss:0.22886386622165755\n",
      "train loss:0.26745975091275065\n",
      "train loss:0.22438882634931925\n",
      "train loss:0.3304887127737476\n",
      "train loss:0.310654724752118\n",
      "train loss:0.22208730667988658\n",
      "train loss:0.23033880674208138\n",
      "train loss:0.26629838307958537\n",
      "train loss:0.25678644835990694\n",
      "train loss:0.18732321159963156\n",
      "train loss:0.2515474803223688\n",
      "train loss:0.29037696051146267\n",
      "train loss:0.19636902946592139\n",
      "train loss:0.4292795893850888\n",
      "train loss:0.3563305187787331\n",
      "train loss:0.26684202035213145\n",
      "train loss:0.282400016239004\n",
      "train loss:0.31198453864716735\n",
      "train loss:0.3042696757563507\n",
      "train loss:0.2501596225652377\n",
      "train loss:0.26242666893690964\n",
      "train loss:0.2711166133767811\n",
      "train loss:0.2589290300790756\n",
      "train loss:0.32062142447209413\n",
      "train loss:0.2630798252551423\n",
      "train loss:0.29852158893791414\n",
      "train loss:0.2533477274893146\n",
      "train loss:0.15896292753581295\n",
      "train loss:0.2570760650759387\n",
      "train loss:0.32318587571787866\n",
      "train loss:0.31673013929612565\n",
      "train loss:0.19579946597279674\n",
      "train loss:0.19637348620437967\n",
      "train loss:0.2840432640489993\n",
      "train loss:0.28220263498247194\n",
      "train loss:0.5061863437477315\n",
      "train loss:0.12499137137837094\n",
      "train loss:0.23455558470599908\n",
      "train loss:0.21851799355985854\n",
      "train loss:0.22778978253171026\n",
      "train loss:0.28970060626843536\n",
      "train loss:0.2603111696934487\n",
      "train loss:0.16053883112256911\n",
      "train loss:0.24504595091655246\n",
      "train loss:0.22814669450622074\n",
      "train loss:0.3490694984583959\n",
      "train loss:0.2575391859048573\n",
      "train loss:0.2153093311973011\n",
      "train loss:0.22521300362144292\n",
      "train loss:0.18278001158687476\n",
      "train loss:0.22613967576927152\n",
      "train loss:0.1806105533778315\n",
      "train loss:0.17276849499102057\n",
      "train loss:0.28149177704713607\n",
      "train loss:0.1820423036080836\n",
      "train loss:0.2504776347403942\n",
      "train loss:0.25213288085562074\n",
      "train loss:0.4202851727077392\n",
      "train loss:0.2307506723511823\n",
      "train loss:0.22971838136129782\n",
      "train loss:0.2196235372864657\n",
      "train loss:0.2266370372343977\n",
      "train loss:0.2776751295105552\n",
      "train loss:0.16013028736368473\n",
      "train loss:0.18352887530803005\n",
      "train loss:0.15591365147670727\n",
      "train loss:0.28982227529537935\n",
      "train loss:0.250746743235102\n",
      "train loss:0.31449712806620017\n",
      "train loss:0.30397095859669676\n",
      "train loss:0.25623243622023373\n",
      "train loss:0.3378462484620868\n",
      "train loss:0.322095452386773\n",
      "train loss:0.20687979057026237\n",
      "train loss:0.21421052661265488\n",
      "train loss:0.26539581034833365\n",
      "train loss:0.19402723461947333\n",
      "train loss:0.18853609264407725\n",
      "train loss:0.35114214478157385\n",
      "train loss:0.2784011364774986\n",
      "train loss:0.1282654768500346\n",
      "train loss:0.372523043341693\n",
      "train loss:0.1936855653485664\n",
      "train loss:0.2209922523884201\n",
      "train loss:0.1922782548302007\n",
      "train loss:0.20437406831775606\n",
      "train loss:0.21560972901969858\n",
      "train loss:0.1373489180736783\n",
      "train loss:0.1352518671448902\n",
      "train loss:0.11536465638433378\n",
      "train loss:0.1761060805028927\n",
      "train loss:0.12819634219686807\n",
      "train loss:0.17235244512179715\n",
      "train loss:0.33691931413316395\n",
      "train loss:0.2025204455549086\n",
      "train loss:0.251127769889498\n",
      "train loss:0.2614123748937964\n",
      "train loss:0.23493985677448972\n",
      "train loss:0.3066214110707189\n",
      "train loss:0.22625687397060862\n",
      "train loss:0.20279706560722205\n",
      "train loss:0.23356731440365128\n",
      "train loss:0.3621342080203609\n",
      "train loss:0.2689930574176953\n",
      "train loss:0.20945635685614228\n",
      "train loss:0.1458081823672273\n",
      "train loss:0.18846574431014415\n",
      "train loss:0.1331930503452748\n",
      "train loss:0.22698500774982525\n",
      "train loss:0.15721295252959336\n",
      "train loss:0.1458758193780209\n",
      "train loss:0.14266078187466713\n",
      "train loss:0.2013494083169336\n",
      "train loss:0.19803058454391173\n",
      "train loss:0.13393605261711478\n",
      "train loss:0.12942656066282768\n",
      "train loss:0.2386587631080273\n",
      "train loss:0.22060909583714838\n",
      "train loss:0.15135068923371803\n",
      "train loss:0.10467959016983436\n",
      "train loss:0.1155737351870574\n",
      "train loss:0.13013918265986604\n",
      "train loss:0.20203074169456248\n",
      "train loss:0.15695482736665084\n",
      "train loss:0.2334052632393167\n",
      "train loss:0.16269926075428306\n",
      "train loss:0.08996382991820834\n",
      "train loss:0.20381628392371154\n",
      "train loss:0.14900261526987643\n",
      "train loss:0.1976851156024049\n",
      "train loss:0.22246574857456594\n",
      "train loss:0.1617654402307157\n",
      "train loss:0.258528572028239\n",
      "train loss:0.12541221636747368\n",
      "train loss:0.19386394238623444\n",
      "train loss:0.1693578095184159\n",
      "train loss:0.15984950458592054\n",
      "train loss:0.19602533902365635\n",
      "train loss:0.13044500112642274\n",
      "train loss:0.2672480674485763\n",
      "train loss:0.2299638970880844\n",
      "train loss:0.1898456659945991\n",
      "train loss:0.13846056190486972\n",
      "train loss:0.22904358503511632\n",
      "train loss:0.2926875195167532\n",
      "train loss:0.21404807457614605\n",
      "train loss:0.1131533476246443\n",
      "train loss:0.29572046931302576\n",
      "train loss:0.1585059595092505\n",
      "train loss:0.22438741040776752\n",
      "train loss:0.2860262970379759\n",
      "train loss:0.0781955383625836\n",
      "train loss:0.148168004010227\n",
      "train loss:0.29250783019405585\n",
      "train loss:0.17072226148288785\n",
      "train loss:0.11297280685831763\n",
      "train loss:0.2799869158542141\n",
      "train loss:0.16193292188202843\n",
      "train loss:0.13346990313255913\n",
      "train loss:0.0928719092306037\n",
      "train loss:0.14191530609582387\n",
      "train loss:0.1762159243647142\n",
      "train loss:0.1143180814782016\n",
      "train loss:0.13801123918782243\n",
      "train loss:0.34071113775594375\n",
      "train loss:0.11652599045879124\n",
      "train loss:0.11771032591939729\n",
      "train loss:0.15675593956480216\n",
      "train loss:0.2033564293152574\n",
      "train loss:0.14461359485074818\n",
      "train loss:0.30446219139796804\n",
      "train loss:0.11660292315518163\n",
      "train loss:0.35430030921347144\n",
      "train loss:0.20483447765400847\n",
      "train loss:0.24363518044385085\n",
      "train loss:0.34930095520000654\n",
      "train loss:0.21757094717916575\n",
      "train loss:0.1284672469926591\n",
      "train loss:0.18722979634208894\n",
      "train loss:0.24720115724415323\n",
      "train loss:0.2759928198788393\n",
      "train loss:0.18039710149641228\n",
      "train loss:0.2080567301613894\n",
      "train loss:0.20890126762569966\n",
      "train loss:0.12992415859470527\n",
      "train loss:0.2435315898275304\n",
      "train loss:0.18275782452028022\n",
      "train loss:0.14658435488664592\n",
      "train loss:0.12883343757457824\n",
      "train loss:0.21736223104646465\n",
      "train loss:0.22819239409718572\n",
      "train loss:0.13498737128771768\n",
      "train loss:0.15046945540092543\n",
      "train loss:0.16801319292047912\n",
      "train loss:0.1626713694802163\n",
      "train loss:0.16539716459186846\n",
      "train loss:0.2128776438747046\n",
      "train loss:0.2467794296371204\n",
      "train loss:0.22536046678689817\n",
      "train loss:0.14862882540207709\n",
      "train loss:0.16988987026027058\n",
      "train loss:0.10207300186441765\n",
      "train loss:0.10627063474192466\n",
      "train loss:0.124269136766509\n",
      "train loss:0.28297040506554005\n",
      "train loss:0.18297980641573058\n",
      "train loss:0.12605006410404077\n",
      "train loss:0.20940440873455127\n",
      "train loss:0.08069246688418902\n",
      "train loss:0.20643893052514095\n",
      "train loss:0.1801769775469003\n",
      "train loss:0.239935998436511\n",
      "train loss:0.10205708100758532\n",
      "train loss:0.10411224691816746\n",
      "train loss:0.12480027445839151\n",
      "train loss:0.3046308017910391\n",
      "train loss:0.22893050674838325\n",
      "train loss:0.1452021897701729\n",
      "train loss:0.21386513776440938\n",
      "train loss:0.19127865956409684\n",
      "train loss:0.14291396878078153\n",
      "train loss:0.13341662876957472\n",
      "train loss:0.103150804637199\n",
      "train loss:0.16722113773902866\n",
      "train loss:0.15344736709581647\n",
      "train loss:0.13612337415928044\n",
      "train loss:0.1383854474623871\n",
      "train loss:0.11485299213974073\n",
      "train loss:0.06264493442757821\n",
      "train loss:0.08821896042520316\n",
      "train loss:0.23750622374274877\n",
      "train loss:0.30062628611519815\n",
      "train loss:0.08225179595136532\n",
      "train loss:0.13480135555722922\n",
      "train loss:0.17342158680243064\n",
      "train loss:0.22590706862146373\n",
      "train loss:0.1520248684795282\n",
      "train loss:0.11401927215827089\n",
      "train loss:0.13428052576156635\n",
      "train loss:0.09319861486559698\n",
      "train loss:0.23228125773308\n",
      "train loss:0.23072312576799306\n",
      "train loss:0.10076673500179104\n",
      "train loss:0.11927818744908457\n",
      "train loss:0.23129365336654875\n",
      "train loss:0.1779237250865183\n",
      "train loss:0.18645144297584504\n",
      "train loss:0.14889797162121865\n",
      "train loss:0.16595113515702042\n",
      "train loss:0.08979652348059972\n",
      "train loss:0.1691494952590941\n",
      "train loss:0.19678168485136857\n",
      "train loss:0.13134377211926654\n",
      "train loss:0.0945332551047713\n",
      "train loss:0.06615911528444876\n",
      "train loss:0.12986991741504544\n",
      "train loss:0.1647376394476724\n",
      "train loss:0.12350416042415806\n",
      "train loss:0.1989572070299734\n",
      "train loss:0.12265634455906228\n",
      "train loss:0.24412976862549715\n",
      "train loss:0.10327841735592332\n",
      "train loss:0.13350773046159292\n",
      "train loss:0.1315362057165553\n",
      "train loss:0.2267546810187817\n",
      "train loss:0.11920943610302562\n",
      "train loss:0.2563709522310492\n",
      "train loss:0.10355633910592672\n",
      "train loss:0.20599729494140306\n",
      "train loss:0.07439974200044444\n",
      "train loss:0.06777183192737364\n",
      "train loss:0.18993501930597578\n",
      "train loss:0.11924931378998989\n",
      "train loss:0.1555474702015823\n",
      "train loss:0.0981221004962536\n",
      "train loss:0.11823518086838165\n",
      "train loss:0.10206452616495298\n",
      "train loss:0.14235723620353125\n",
      "train loss:0.08875789362654234\n",
      "train loss:0.14280161303958402\n",
      "train loss:0.06412734181764357\n",
      "train loss:0.11509888602405431\n",
      "train loss:0.1592956829808413\n",
      "train loss:0.06884458927795739\n",
      "train loss:0.09761846912109702\n",
      "train loss:0.13326407014374866\n",
      "train loss:0.09140201874245271\n",
      "train loss:0.16518983767922013\n",
      "train loss:0.07553919820014621\n",
      "train loss:0.17807055283311324\n",
      "train loss:0.26213786214409224\n",
      "train loss:0.1283098674030348\n",
      "train loss:0.12838377010810073\n",
      "train loss:0.19754375003686428\n",
      "train loss:0.15984962135231492\n",
      "train loss:0.16307791398056914\n",
      "train loss:0.07147818167217936\n",
      "train loss:0.22316360433830187\n",
      "train loss:0.12636857687377961\n",
      "train loss:0.10481282440846268\n",
      "train loss:0.13584721225262766\n",
      "train loss:0.10933557211599904\n",
      "train loss:0.16263214630407827\n",
      "train loss:0.13006232156366587\n",
      "train loss:0.10318486592620416\n",
      "train loss:0.13596939384424525\n",
      "train loss:0.07965712185680686\n",
      "train loss:0.21944718283982037\n",
      "train loss:0.12025664890909915\n",
      "train loss:0.10357068847659119\n",
      "train loss:0.13944522511335355\n",
      "train loss:0.1036200250685395\n",
      "train loss:0.2599034008548313\n",
      "train loss:0.2095408657536154\n",
      "train loss:0.15392132596460123\n",
      "train loss:0.18827641469002493\n",
      "train loss:0.16415851096514955\n",
      "train loss:0.17058851179209789\n",
      "train loss:0.13279460996561562\n",
      "train loss:0.13209351060562502\n",
      "train loss:0.04715207846380702\n",
      "train loss:0.14113444093959063\n",
      "train loss:0.07649531160458689\n",
      "train loss:0.13256779148947412\n",
      "train loss:0.08900929802366625\n",
      "train loss:0.07178523700952906\n",
      "train loss:0.2611913294566906\n",
      "train loss:0.1656789154092836\n",
      "train loss:0.17327899333704774\n",
      "train loss:0.19358111822533008\n",
      "train loss:0.17018526763480885\n",
      "train loss:0.09785388992124508\n",
      "train loss:0.1833582722544703\n",
      "train loss:0.06287960562126634\n",
      "train loss:0.17844968454175575\n",
      "train loss:0.17661671533155482\n",
      "train loss:0.09760870879322671\n",
      "train loss:0.12834724740195694\n",
      "train loss:0.1691444595013268\n",
      "train loss:0.12553481115506412\n",
      "train loss:0.1665302723541647\n",
      "train loss:0.20085212539688602\n",
      "train loss:0.16458456221003245\n",
      "train loss:0.13177747418278757\n",
      "train loss:0.08910478140360781\n",
      "train loss:0.16898662433343406\n",
      "train loss:0.07627102474127023\n",
      "train loss:0.12703150846135003\n",
      "train loss:0.10885671990800104\n",
      "train loss:0.10685465422275765\n",
      "train loss:0.08218978172599928\n",
      "train loss:0.04436858662914672\n",
      "train loss:0.1282345075434962\n",
      "train loss:0.06303852427820657\n",
      "train loss:0.12471192587330743\n",
      "train loss:0.20525179692817364\n",
      "train loss:0.2250159895922671\n",
      "train loss:0.18531376313369247\n",
      "train loss:0.11098962908151011\n",
      "train loss:0.11513769583468278\n",
      "train loss:0.09091133044839823\n",
      "train loss:0.10921240919898728\n",
      "train loss:0.19843746719539357\n",
      "train loss:0.15027569226080298\n",
      "train loss:0.1287903820031683\n",
      "train loss:0.228412436680943\n",
      "train loss:0.2071053160033044\n",
      "train loss:0.08181120050803489\n",
      "train loss:0.06353901738523063\n",
      "train loss:0.14317432476554992\n",
      "train loss:0.1621417105742702\n",
      "train loss:0.09741407271427148\n",
      "train loss:0.10797875203859394\n",
      "train loss:0.17991914453141328\n",
      "train loss:0.2817571675521422\n",
      "train loss:0.1307960082388257\n",
      "train loss:0.08261791995633183\n",
      "train loss:0.08009697477415449\n",
      "train loss:0.05471035781879697\n",
      "train loss:0.0865887428270789\n",
      "train loss:0.18377398990940638\n",
      "train loss:0.1654060063681588\n",
      "train loss:0.19910387072113522\n",
      "train loss:0.16386639820496618\n",
      "train loss:0.09014832526018651\n",
      "train loss:0.07193891730886469\n",
      "train loss:0.09016898907826375\n",
      "train loss:0.08030519847325329\n",
      "train loss:0.09235577217703629\n",
      "train loss:0.12965389953886294\n",
      "train loss:0.08993205141507596\n",
      "train loss:0.0812669764934977\n",
      "train loss:0.16949545406183014\n",
      "train loss:0.1128624561041841\n",
      "train loss:0.09572518409250169\n",
      "train loss:0.1589901001775475\n",
      "train loss:0.13270434311985757\n",
      "train loss:0.0712550225085983\n",
      "train loss:0.08873683313380797\n",
      "train loss:0.08764975878624956\n",
      "train loss:0.07836895842735057\n",
      "train loss:0.16502320583991725\n",
      "=== epoch:2, train acc:0.962, test acc:0.963 ===\n",
      "train loss:0.06689857463816304\n",
      "train loss:0.09496807328658945\n",
      "train loss:0.07019434830386244\n",
      "train loss:0.1129331047988626\n",
      "train loss:0.15284407307155146\n",
      "train loss:0.1269513565511754\n",
      "train loss:0.05763903182912736\n",
      "train loss:0.18484574532655873\n",
      "train loss:0.10877875748771972\n",
      "train loss:0.04469049691951407\n",
      "train loss:0.1401075865213177\n",
      "train loss:0.06335341051231555\n",
      "train loss:0.11076817419906099\n",
      "train loss:0.07984626957779818\n",
      "train loss:0.2021145850421633\n",
      "train loss:0.06624072615213614\n",
      "train loss:0.09119314690299354\n",
      "train loss:0.11457479591244113\n",
      "train loss:0.10640346632395073\n",
      "train loss:0.11427056804421173\n",
      "train loss:0.15533673044560062\n",
      "train loss:0.09164208319513963\n",
      "train loss:0.1818894500480246\n",
      "train loss:0.13883195148526298\n",
      "train loss:0.15136585111816156\n",
      "train loss:0.16112474516917133\n",
      "train loss:0.1445923924249736\n",
      "train loss:0.08818364532652927\n",
      "train loss:0.14723075190674256\n",
      "train loss:0.17203725624061356\n",
      "train loss:0.06130100482659356\n",
      "train loss:0.09584006667955496\n",
      "train loss:0.11211670603974405\n",
      "train loss:0.15189548865522717\n",
      "train loss:0.0923095882414379\n",
      "train loss:0.12543006662436523\n",
      "train loss:0.0876454074954696\n",
      "train loss:0.04760023737398106\n",
      "train loss:0.14105835720982415\n",
      "train loss:0.10079321348675183\n",
      "train loss:0.1014809310244038\n",
      "train loss:0.1696666963018711\n",
      "train loss:0.14793223044467313\n",
      "train loss:0.10468281763219618\n",
      "train loss:0.1393006822597671\n",
      "train loss:0.1336327860085851\n",
      "train loss:0.15760747477408013\n",
      "train loss:0.1045395091105336\n",
      "train loss:0.1851975334745195\n",
      "train loss:0.061789312294552444\n",
      "train loss:0.05310073883246977\n",
      "train loss:0.0861257730434375\n",
      "train loss:0.09587295591313452\n",
      "train loss:0.1160653036342177\n",
      "train loss:0.1561126103277385\n",
      "train loss:0.09720360870275105\n",
      "train loss:0.08544062555908058\n",
      "train loss:0.10183616324922852\n",
      "train loss:0.07621665787551725\n",
      "train loss:0.16495486090314745\n",
      "train loss:0.07658801928128488\n",
      "train loss:0.1300358532160524\n",
      "train loss:0.11543590269551254\n",
      "train loss:0.15730362950224241\n",
      "train loss:0.04976626846700656\n",
      "train loss:0.053293904410303246\n",
      "train loss:0.10526704374993111\n",
      "train loss:0.11204636145264503\n",
      "train loss:0.1533258377155482\n",
      "train loss:0.15315156195225327\n",
      "train loss:0.08203372671269998\n",
      "train loss:0.24852827559618132\n",
      "train loss:0.28251707928308023\n",
      "train loss:0.16106198018777249\n",
      "train loss:0.1118198692279368\n",
      "train loss:0.09369650215207109\n",
      "train loss:0.10435302329339011\n",
      "train loss:0.15034940205959382\n",
      "train loss:0.16871292456906\n",
      "train loss:0.09328855903034093\n",
      "train loss:0.18555107587513092\n",
      "train loss:0.1642525310560162\n",
      "train loss:0.26306228553341976\n",
      "train loss:0.10406616182575906\n",
      "train loss:0.09243006723164299\n",
      "train loss:0.10422573138500901\n",
      "train loss:0.133927228533779\n",
      "train loss:0.26410154769605787\n",
      "train loss:0.07355291849775095\n",
      "train loss:0.2675044234555514\n",
      "train loss:0.08385984257109488\n",
      "train loss:0.1292964918902878\n",
      "train loss:0.13537592564942047\n",
      "train loss:0.13936144640405396\n",
      "train loss:0.10576548034276137\n",
      "train loss:0.13745231310353126\n",
      "train loss:0.0800221114814721\n",
      "train loss:0.1832089472201842\n",
      "train loss:0.16311213225649676\n",
      "train loss:0.12598922467016352\n",
      "train loss:0.06183860867285988\n",
      "train loss:0.07892366603353321\n",
      "train loss:0.10603837691934782\n",
      "train loss:0.0760479947310444\n",
      "train loss:0.187655139363137\n",
      "train loss:0.2617822313399221\n",
      "train loss:0.06198650704766839\n",
      "train loss:0.09180203150858667\n",
      "train loss:0.11478102522918579\n",
      "train loss:0.2505391288361269\n",
      "train loss:0.11238428274328849\n",
      "train loss:0.0906809300982505\n",
      "train loss:0.11209355691321793\n",
      "train loss:0.14944788999234132\n",
      "train loss:0.1765607765636728\n",
      "train loss:0.13364899452089896\n",
      "train loss:0.05229032062468047\n",
      "train loss:0.13187707018251854\n",
      "train loss:0.1150292972281034\n",
      "train loss:0.11542074098433386\n",
      "train loss:0.08228267791217786\n",
      "train loss:0.10024451331070637\n",
      "train loss:0.08009742961694628\n",
      "train loss:0.07658518505694271\n",
      "train loss:0.1028678237370852\n",
      "train loss:0.08743164935488051\n",
      "train loss:0.12962883006101522\n",
      "train loss:0.0692092212525136\n",
      "train loss:0.09777345493839691\n",
      "train loss:0.08209221380509485\n",
      "train loss:0.08847517119122003\n",
      "train loss:0.06915542681676767\n",
      "train loss:0.1296137444245832\n",
      "train loss:0.09730459795690463\n",
      "train loss:0.06412174739300489\n",
      "train loss:0.06437864707794665\n",
      "train loss:0.15899615808868708\n",
      "train loss:0.2049897378362799\n",
      "train loss:0.055076631147087454\n",
      "train loss:0.14220223021654724\n",
      "train loss:0.08726150814849762\n",
      "train loss:0.052688403752180424\n",
      "train loss:0.06455463511347555\n",
      "train loss:0.14468817718403218\n",
      "train loss:0.07226071614213114\n",
      "train loss:0.07544477919637542\n",
      "train loss:0.0710081854725488\n",
      "train loss:0.08890812224174063\n",
      "train loss:0.10988093192316507\n",
      "train loss:0.11466095723272474\n",
      "train loss:0.22603289933808543\n",
      "train loss:0.08290811209480171\n",
      "train loss:0.13368727840010533\n",
      "train loss:0.09499228190588874\n",
      "train loss:0.19107743830108848\n",
      "train loss:0.19643674582196627\n",
      "train loss:0.1023089235715806\n",
      "train loss:0.07799379509449426\n",
      "train loss:0.06161272176542118\n",
      "train loss:0.09859473699118797\n",
      "train loss:0.04234208510858537\n",
      "train loss:0.11691635957247704\n",
      "train loss:0.05813446260907613\n",
      "train loss:0.0334371791518264\n",
      "train loss:0.08049163358583136\n",
      "train loss:0.07783545238408761\n",
      "train loss:0.04301989712633274\n",
      "train loss:0.08493428599689096\n",
      "train loss:0.08011781329028692\n",
      "train loss:0.07619164145879725\n",
      "train loss:0.0650018003228625\n",
      "train loss:0.041708120468749534\n",
      "train loss:0.08201869631771838\n",
      "train loss:0.12222279367723578\n",
      "train loss:0.11844594237844194\n",
      "train loss:0.08570660576091424\n",
      "train loss:0.07379144381329782\n",
      "train loss:0.06609152952034605\n",
      "train loss:0.07836440997780461\n",
      "train loss:0.11977676858454821\n",
      "train loss:0.15748873298719362\n",
      "train loss:0.05809394250586317\n",
      "train loss:0.08886605329126052\n",
      "train loss:0.0817242070393145\n",
      "train loss:0.07266927987096722\n",
      "train loss:0.21875187397272378\n",
      "train loss:0.1024095552372619\n",
      "train loss:0.037754159584549064\n",
      "train loss:0.08314724203767117\n",
      "train loss:0.18470990540692506\n",
      "train loss:0.2016137231182636\n",
      "train loss:0.15079827900602338\n",
      "train loss:0.051156066952918994\n",
      "train loss:0.0679641744723556\n",
      "train loss:0.045615472245895905\n",
      "train loss:0.1530918395652558\n",
      "train loss:0.08831242897580097\n",
      "train loss:0.05869235516568194\n",
      "train loss:0.17504190863374927\n",
      "train loss:0.07406642988560554\n",
      "train loss:0.031208032515551026\n",
      "train loss:0.08559111546903095\n",
      "train loss:0.03312443707818647\n",
      "train loss:0.15392814586675357\n",
      "train loss:0.04665853083750534\n",
      "train loss:0.04740233020465027\n",
      "train loss:0.09430817938199337\n",
      "train loss:0.13248909712073054\n",
      "train loss:0.04235140650228106\n",
      "train loss:0.06715266384736772\n",
      "train loss:0.13292350466868016\n",
      "train loss:0.16668826529452865\n",
      "train loss:0.07444445125241062\n",
      "train loss:0.09819218022901383\n",
      "train loss:0.11328783208759306\n",
      "train loss:0.17416440442904682\n",
      "train loss:0.07000471692629186\n",
      "train loss:0.13103460244974774\n",
      "train loss:0.16270764333709206\n",
      "train loss:0.07100635856582053\n",
      "train loss:0.03385816210296232\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 26\u001B[0m\n\u001B[1;32m     18\u001B[0m network \u001B[38;5;241m=\u001B[39m SimpleConvNet(input_dim\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m28\u001B[39m,\u001B[38;5;241m28\u001B[39m),\n\u001B[1;32m     19\u001B[0m                         conv_param \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilter_num\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m30\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilter_size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m5\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpad\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstride\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1\u001B[39m},\n\u001B[1;32m     20\u001B[0m                         hidden_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, output_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, weight_init_std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m     22\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(network, x_train, t_train, x_test, t_test,\n\u001B[1;32m     23\u001B[0m                   epochs\u001B[38;5;241m=\u001B[39mmax_epochs, mini_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,\n\u001B[1;32m     24\u001B[0m                   optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAdam\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer_param\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.001\u001B[39m},\n\u001B[1;32m     25\u001B[0m                   evaluate_sample_num_per_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m---> 26\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# 매개변수 보존\u001B[39;00m\n\u001B[1;32m     29\u001B[0m network\u001B[38;5;241m.\u001B[39msave_params(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/deep_learning_from_scratch/1권/common/trainer.py:76\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_iter):\n\u001B[0;32m---> 76\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m     test_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39maccuracy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx_test, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mt_test)\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose:\n",
      "File \u001B[0;32m~/Documents/GitHub/deep_learning_from_scratch/1권/common/trainer.py:50\u001B[0m, in \u001B[0;36mTrainer.train_step\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     47\u001B[0m grads \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mgradient(x_batch, t_batch)\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mparams, grads)\n\u001B[0;32m---> 50\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_loss_list\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(loss))\n",
      "Cell \u001B[0;32mIn[19], line 74\u001B[0m, in \u001B[0;36mSimpleConvNet.loss\u001B[0;34m(self, x, t)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, t):\n\u001B[1;32m     67\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"손실 함수를 구한다.\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \n\u001B[1;32m     69\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;124;03m    t : 정답 레이블\u001B[39;00m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 74\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_layer\u001B[38;5;241m.\u001B[39mforward(y, t)\n",
      "Cell \u001B[0;32mIn[19], line 62\u001B[0m, in \u001B[0;36mSimpleConvNet.predict\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m---> 62\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/Documents/GitHub/deep_learning_from_scratch/1권/common/layers.py:57\u001B[0m, in \u001B[0;36mAffine.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     54\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m x\n\u001B[0;32m---> 57\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28),\n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
